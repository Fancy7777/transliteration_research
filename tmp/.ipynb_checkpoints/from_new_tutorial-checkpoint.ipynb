{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get in the data\n",
    "chinese = []\n",
    "english_phoeneme = []\n",
    "with open('dataset.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        chinese.append(temp[1])\n",
    "        english_phoeneme.append(temp[2])\n",
    "english_phoeneme = [phoeneme.split('\\n')[0].split() for phoeneme in english_phoeneme]\n",
    "chinese = [list(word) for word in chinese]\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')\n",
    "def prepare_vocab_input():\n",
    "    vocab_inputs = []\n",
    "    with open('./english_phoneme_vocabulary_output.txt') as file:\n",
    "        for line in file:\n",
    "            vocab_inputs.append(line.split('\\n')[0])\n",
    "    vocab_inputs.remove('_PAD')\n",
    "    vocab_inputs.remove('_GO')\n",
    "    vocab_inputs.remove('_EOS')\n",
    "    vocab_inputs.remove('_UNK')\n",
    "    #vocab_inputs = ['PAD', 'EOS'] + (vocab_inputs)\n",
    "    vocab_inputs =  (vocab_inputs)+['PAD']\n",
    "    return vocab_inputs\n",
    "def prepare_vocab_predict():\n",
    "    vocab_predict = list(chinese_id_dict.keys())\n",
    "    return vocab_predict\n",
    "# prepare chinese_id_dict\n",
    "chinese_id_list = []\n",
    "with open('./chinese_vocabulary.txt') as file7:\n",
    "    for line in file7:\n",
    "        chinese_id_list.append(line.split('\\n')[0])\n",
    "chinese_id_list =  chinese_id_list+['PAD']\n",
    "chinese_id_dict = {}\n",
    "for i in range(len(chinese_id_list)):\n",
    "    chinese_id_dict[chinese_id_list[i]] = i\n",
    "# finish preparation for chinese_id_dict\n",
    "vocab_inputs = prepare_vocab_input()\n",
    "vocab_predict = prepare_vocab_predict()\n",
    "english_phoneme_dict = {}\n",
    "for i in range(len(vocab_inputs)):\n",
    "    english_phoneme_dict[vocab_inputs[i]] = i\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phoneme_dict - char2numX\n",
    "chinese_id_dict - char2numY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-9bc6acc26643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#further prepared it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menglish_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menglish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoneme\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphoneme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_now\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-9bc6acc26643>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#further prepared it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menglish_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menglish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PAD'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menglish_phoneme_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoneme\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mphoneme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_now\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "#further prepared it\n",
    "english_now = dict(zip(english_phoneme_dict.values(),english_phoneme_dict.keys()))\n",
    "english = [[english_phoneme_dict['PAD']]+[english_phoneme_dict[phoneme] for phoneme in ele] for ele in english_now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AA': 10,\n",
       " 'AE': 2,\n",
       " 'AH': 0,\n",
       " 'AO': 28,\n",
       " 'AW': 33,\n",
       " 'AY': 26,\n",
       " 'B': 12,\n",
       " 'CH': 30,\n",
       " 'D': 7,\n",
       " 'DH': 38,\n",
       " 'EH': 14,\n",
       " 'ER': 25,\n",
       " 'EY': 16,\n",
       " 'F': 24,\n",
       " 'G': 19,\n",
       " 'HH': 22,\n",
       " 'IH': 13,\n",
       " 'IY': 5,\n",
       " 'JH': 21,\n",
       " 'K': 8,\n",
       " 'L': 4,\n",
       " 'M': 6,\n",
       " 'N': 1,\n",
       " 'NG': 32,\n",
       " 'OW': 15,\n",
       " 'OY': 36,\n",
       " 'P': 23,\n",
       " 'PAD': 39,\n",
       " 'R': 3,\n",
       " 'S': 9,\n",
       " 'SH': 31,\n",
       " 'T': 11,\n",
       " 'TH': 34,\n",
       " 'UH': 35,\n",
       " 'UW': 20,\n",
       " 'V': 18,\n",
       " 'W': 29,\n",
       " 'Y': 27,\n",
       " 'Z': 17,\n",
       " 'ZH': 37}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_phoneme_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#128,256\n",
    "batch_size = 256\n",
    "embedding_size = 256\n",
    "encoder_hidden_units = 100\n",
    "decoder_hidden_units = 100\n",
    "epoch = 100\n",
    "encoder_max_length = 39\n",
    "decoder_max_length =75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tensor where we will feed the data into graph\n",
    "encoder_inputs = tf.placeholder(shape = (None, encoder_max_length), dtype = tf.int32)\n",
    "decoder_targets = tf.placeholder(shape = (None, decoder_max_length+1), dtype = tf.int32)\n",
    "decoder_inputs = tf.placeholder(shape = (None, decoder_max_length+1), dtype = tf.int32)\n",
    "# Embedding layers\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_inputs), embedding_size]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_predict), embedding_size]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(encoder_embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(decoder_embeddings, decoder_inputs)\n",
    "\n",
    "\n",
    "## encoder layer\n",
    "encoder_cell = tf.contrib.rnn.BasicLSTMCell(encoder_hidden_units)\n",
    "_, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded, dtype=tf.float32)\n",
    "\n",
    "    \n",
    "## decoding layer\n",
    "#with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "decoder_cell = tf.contrib.rnn.BasicLSTMCell(decoder_hidden_units,reuse=True)\n",
    "decoder_outputs, _ = tf.nn.dynamic_rnn(decoder_cell, decoder_inputs_embedded, initial_state=encoder_final_state)\n",
    "\n",
    "##connect outputs to \n",
    "logits = tf.contrib.layers.fully_connected(decoder_outputs, len(vocab_predict)) \n",
    "#with tf.name_scope(\"optimization\"):\n",
    "# Loss function\n",
    "def sent_len(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "        sentence: [None, max_sentence_length]\n",
    "    Returns:\n",
    "        length: [None], the actual length of each sentence in the batch\n",
    "    '''\n",
    "    used = tf.sign(tf.abs(sentence))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(x=length, dtype=tf.int32)\n",
    "    return length\n",
    "\n",
    "cross_entropy_mask = tf.sequence_mask(\n",
    "    lengths=sent_len(decoder_targets),\n",
    "    maxlen=decoder_max_length + 1,\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=logits, # [None, max_sentence_length, tag_set_size]\n",
    "    targets=decoder_targets, # [None, max_sentence_length]\n",
    "    weights=cross_entropy_mask,\n",
    "    average_across_timesteps=True,\n",
    "    average_across_batch=True,\n",
    "    name=\"seq2seq_sequence_loss\",\n",
    ")\n",
    "\n",
    "#prediction\n",
    "decoder_prediction = tf.argmax(logits, 2)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(inputs, max_sequence_length = None):\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape = [batch_size, max_sequence_length], dtype = np.int32)\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_batch_major, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feeding(inputs, labels):\n",
    "    inputs_int = []; predict_int = []\n",
    "    for i in range(len(inputs)):\n",
    "        single_input = []\n",
    "        single_predict = []\n",
    "        for x in range(encoder_max_length):\n",
    "            try:\n",
    "                single_input += [english_phoneme_dict[inputs[i][x]]]\n",
    "            except:\n",
    "                single_input += [0]\n",
    "        for x in range(decoder_max_length):\n",
    "            try:\n",
    "                single_predict += [chinese_id_dict[labels[i][x]]]\n",
    "            except:\n",
    "                single_predict += [0]\n",
    "        inputs_int.append(single_input); predict_int.append(single_predict)\n",
    "            \n",
    "    enc_input, _ = batch(inputs_int)\n",
    "    dec_target, _ = batch([(sequence) + [1] for sequence in predict_int])\n",
    "    dec_input, _ = batch([[1] + (sequence) for sequence in predict_int])\n",
    "    #print(enc_input.shape)\n",
    "    return {encoder_inputs: enc_input, decoder_inputs: dec_input, decoder_targets: dec_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_chinese(label):\n",
    "    chinese = ''\n",
    "    for i in range(len(label)):\n",
    "        for j in range(len(label[i])):\n",
    "            if label[i][j] == 0 or label[i][j] == 1:\n",
    "                continue\n",
    "            chinese += vocab_predict[label[i][j]] + ' '\n",
    "    return chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-f7d187ca1219>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         _, losses = sess.run([optimizer, loss], \n\u001b[0;32m---> 18\u001b[0;31m                              feeding(batch_english_phoneme[w: w + batch_size], batch_chinese[w: w + batch_size]))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "for q in range(epoch):\n",
    "    total_loss = 0\n",
    "    lasttime = time.time()\n",
    "    \n",
    "    for w in range(0, len(english_phoeneme) - batch_size, batch_size):  \n",
    "        \n",
    "        #shuffle the batch\n",
    "        combine = list(zip(english_phoeneme,chinese))\n",
    "        random.shuffle(combine)\n",
    "        batch_english_phoneme,batch_chinese = zip(*combine)\n",
    "        \n",
    "        _, losses = sess.run([optimizer, loss], \n",
    "                             feeding(batch_english_phoneme[w: w + batch_size], batch_chinese[w: w + batch_size]))\n",
    "        \n",
    "        total_loss += losses\n",
    "        \n",
    "    total_loss = total_loss / ((len(english_phoeneme) - batch_size) / (batch_size * 1.0))\n",
    "    LOSS.append(total_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (q + 1) % 10 == 0:\n",
    "        print('epoch: ' + str(q + 1) + ', total loss: ' + str(total_loss) + ', s/epoch: ' + str(time.time() - lasttime))\n",
    "    for i in range(10):\n",
    "        rand = np.random.randint(len(english_phoeneme))\n",
    "        test = feeding(english_phoeneme[rand: rand + 1], chinese[rand: rand + 1])\n",
    "        predict = sess.run(decoder_prediction, test)\n",
    "        print('input: ' + str(english_phoeneme[rand: rand + 1]))\n",
    "        print('supposed label: ' + str(chinese[rand: rand + 1]))\n",
    "        print('predict label:' + str(label_to_chinese(predict)) + '\\n')\n",
    "    print('#######################next 10 epoch#############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = feeding(english_phoeneme[25: 25 + 1], chinese[25: 25 + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = sess.run(decoder_prediction, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'亘 干 喇 方 马 船 闭 闭 闭 额 额 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 则 '"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_chinese(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['阿', '尔', '宾', '松']]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese[25: 25 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
