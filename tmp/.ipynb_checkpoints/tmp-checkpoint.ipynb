{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_targets = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_vocab_input():\n",
    "    vocab_inputs = []\n",
    "    with open('./english_phoneme_vocabulary_output.txt') as file:\n",
    "        for line in file:\n",
    "            vocab_inputs.append(line.split('\\n')[0])\n",
    "    vocab_inputs.remove('_PAD')\n",
    "    vocab_inputs.remove('_GO')\n",
    "    vocab_inputs.remove('_EOS')\n",
    "    vocab_inputs.remove('_UNK')\n",
    "    vocab_inputs = ['PAD', 'EOS'] + (vocab_inputs)\n",
    "    return vocab_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_vocab_predict():\n",
    "    vocab_predict = list(chinese_id_dict.keys())\n",
    "    #vocab_predict.remove('_PAD')\n",
    "    #vocab_predict.remove('_GO')\n",
    "    #vocab_predict.remove('_EOS')\n",
    "    #vocab_predict.remove('_UNK')\n",
    "    vocab_predict = ['PAD', 'EOS'] + (vocab_predict)\n",
    "    return vocab_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # prepare chinese_id_dict\n",
    "    chinese_id_list = []\n",
    "    with open('./chinese_vocabulary.txt') as file7:\n",
    "        for line in file7:\n",
    "            chinese_id_list.append(line.split('\\n')[0])\n",
    "    chinese_id_list = ['PAD', 'EOS'] + chinese_id_list\n",
    "    chinese_id_dict = {}\n",
    "    for i in range(len(chinese_id_list)):\n",
    "        chinese_id_dict[chinese_id_list[i]] = i\n",
    "    # finish preparation for chinese_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inputs = prepare_vocab_input()\n",
    "vocab_predict = prepare_vocab_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAD',\n",
       " 'EOS',\n",
       " 'AH',\n",
       " 'N',\n",
       " 'AE',\n",
       " 'R',\n",
       " 'L',\n",
       " 'IY',\n",
       " 'M',\n",
       " 'D',\n",
       " 'K',\n",
       " 'S',\n",
       " 'AA',\n",
       " 'T',\n",
       " 'B',\n",
       " 'IH',\n",
       " 'EH',\n",
       " 'OW',\n",
       " 'EY',\n",
       " 'Z',\n",
       " 'V',\n",
       " 'G',\n",
       " 'UW',\n",
       " 'JH',\n",
       " 'HH',\n",
       " 'P',\n",
       " 'F',\n",
       " 'ER',\n",
       " 'AY',\n",
       " 'Y',\n",
       " 'AO',\n",
       " 'W',\n",
       " 'CH',\n",
       " 'SH',\n",
       " 'NG',\n",
       " 'AW',\n",
       " 'TH',\n",
       " 'UH',\n",
       " 'OY',\n",
       " 'ZH',\n",
       " 'DH']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fcfb4631ce56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgreatestvalue_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m encoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_inputs), greatestvalue_predict]\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                    , -1.0, 1.0), dtype = tf.float32)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m decoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_predict), greatestvalue_predict]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "greatestvalue_predict = 42\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_inputs), greatestvalue_predict]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_predict), greatestvalue_predict]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(encoder_embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(decoder_embeddings, decoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
