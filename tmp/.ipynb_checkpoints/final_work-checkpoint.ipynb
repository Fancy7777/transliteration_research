{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get in the data\n",
    "chinese = []\n",
    "english_phoeneme = []\n",
    "with open('dataset.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        chinese.append(temp[1])\n",
    "        english_phoeneme.append(temp[2])\n",
    "english_phoeneme = [phoeneme.split('\\n')[0].split() for phoeneme in english_phoeneme]\n",
    "chinese = [list(word) for word in chinese]\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')\n",
    "def prepare_vocab_input():\n",
    "    vocab_inputs = []\n",
    "    with open('./english_phoneme_vocabulary_output.txt') as file:\n",
    "        for line in file:\n",
    "            vocab_inputs.append(line.split('\\n')[0])\n",
    "    vocab_inputs.remove('_PAD')\n",
    "    vocab_inputs.remove('_GO')\n",
    "    vocab_inputs.remove('_EOS')\n",
    "    vocab_inputs.remove('_UNK')\n",
    "    #vocab_inputs = ['PAD', 'EOS'] + (vocab_inputs)\n",
    "    vocab_inputs =  (vocab_inputs)+['PAD']\n",
    "    return vocab_inputs\n",
    "def prepare_vocab_predict():\n",
    "    vocab_predict = list(chinese_id_dict.keys())\n",
    "    return vocab_predict\n",
    "# prepare chinese_id_dict\n",
    "chinese_id_list = []\n",
    "with open('./chinese_vocabulary.txt') as file7:\n",
    "    for line in file7:\n",
    "        chinese_id_list.append(line.split('\\n')[0])\n",
    "chinese_id_list =  chinese_id_list+['PAD']\n",
    "chinese_id_dict = {}\n",
    "for i in range(len(chinese_id_list)):\n",
    "    chinese_id_dict[chinese_id_list[i]] = i\n",
    "# finish preparation for chinese_id_dict\n",
    "vocab_inputs = prepare_vocab_input()\n",
    "vocab_predict = prepare_vocab_predict()\n",
    "english_phoneme_dict = {}\n",
    "for i in range(len(vocab_inputs)):\n",
    "    english_phoneme_dict[vocab_inputs[i]] = i\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#128,256\n",
    "batch_size = 128\n",
    "embedding_size = 256\n",
    "encoder_hidden_units = 32\n",
    "decoder_hidden_units = 32\n",
    "epoch = 10\n",
    "encoder_max_length = 39\n",
    "decoder_max_length =75\n",
    "\n",
    "#placeholders pass data in\n",
    "encoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_targets = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "\n",
    "#variables\n",
    "#size should be [max_time,batch_size] try batch_size here\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_inputs), embedding_size]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_predict), embedding_size]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(encoder_embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(decoder_embeddings, decoder_inputs)\n",
    "\n",
    "with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "    #initial encoder cell to be LSTM\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded,\n",
    "                                                dtype=tf.float32, time_major=False)\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    #initial decoder to be also LSTMcell\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(decoder_cell, decoder_inputs_embedded,\n",
    "                            initial_state=encoder_final_state,dtype=tf.float32, time_major=False,scope='decoder')\n",
    "\n",
    "    \n",
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, len(vocab_predict))\n",
    "# [None, seq_len, vocab_size]\n",
    "\n",
    "#在第二个维度上取argmax\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "def sent_len(sentence):\n",
    "    '''\n",
    "    Args:\n",
    "        sentence: [None, max_sentence_length]\n",
    "    Returns:\n",
    "        length: [None], the actual length of each sentence in the batch\n",
    "    '''\n",
    "    used = tf.sign(tf.abs(sentence))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(x=length, dtype=tf.int32)\n",
    "    return length\n",
    "\n",
    "cross_entropy_mask = tf.sequence_mask(\n",
    "    lengths=sent_len(decoder_targets),\n",
    "    maxlen=decoder_max_length + 1,\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "# [None, max_sentence_length]\n",
    "\n",
    "loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=decoder_logits, # [None, max_sentence_length, tag_set_size]\n",
    "    targets=decoder_targets, # [None, max_sentence_length]\n",
    "    weights=cross_entropy_mask,\n",
    "    average_across_timesteps=True,\n",
    "    average_across_batch=True,\n",
    "    name=\"seq2seq_sequence_loss\",\n",
    ")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(inputs, max_sequence_length = None):\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape = [batch_size, max_sequence_length], dtype = np.int32)\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_batch_major, sequence_lengths\n",
    "\n",
    "def feeding(inputs, labels):\n",
    "    inputs_int = []; predict_int = []\n",
    "    for i in range(len(inputs)):\n",
    "        single_input = []\n",
    "        single_predict = []\n",
    "        for x in range(encoder_max_length):\n",
    "            try:\n",
    "                single_input += [english_phoneme_dict[inputs[i][x]]]\n",
    "            except:\n",
    "                single_input += [0]\n",
    "        for x in range(decoder_max_length):\n",
    "            try:\n",
    "                single_predict += [chinese_id_dict[labels[i][x]]]\n",
    "            except:\n",
    "                single_predict += [0]\n",
    "        inputs_int.append(single_input); predict_int.append(single_predict)\n",
    "            \n",
    "    enc_input, _ = batch(inputs_int)\n",
    "    dec_target, _ = batch([(sequence) + [1] for sequence in predict_int])\n",
    "    dec_input, _ = batch([[1] + (sequence) for sequence in predict_int])\n",
    "    #print(enc_input.shape)\n",
    "    return {encoder_inputs: enc_input, decoder_inputs: dec_input, decoder_targets: dec_target}\n",
    "\n",
    "def label_to_chinese(label):\n",
    "    chinese = ''\n",
    "    for i in range(len(label)):\n",
    "        for j in range(len(label[i])):\n",
    "            if label[i][j] == 0 or label[i][j] == 1:\n",
    "                continue\n",
    "            chinese += vocab_predict[label[i][j]] + ' '\n",
    "    return chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [['K', 'EH', 'S', 'L', 'ER', 'K', 'L', 'EH', 'R', 'M', 'AA', 'N', 'T']]\n",
      "supposed label: [['凯', '斯', '莱', '克', '莱', '蒙']]\n",
      "predict label:阿 瑟 特 斯 \n",
      "\n",
      "input: [['AE', 'Z', 'Z', 'AO', 'IY', 'AY', 'AH']]\n",
      "supposed label: [['扎', '维', '耶']]\n",
      "predict label:阿 尔 尔 维 \n",
      "\n",
      "input: [['M', 'OY', 'Z', 'IH', 'Z', 'AO', 'R', 'OW', 'Z', 'K', 'OW']]\n",
      "supposed label: [['莫', '伊', '塞', '斯', '奥', '罗', '斯', '科']]\n",
      "predict label:阿 里 拉 尔 尔 \n",
      "\n",
      "input: [['S', 'AA', 'D', 'M', 'OW', 'HH', 'AA', 'M', 'EH', 'D', 'B', 'IH', 'N', 'T', 'EH', 'F', 'L', 'AH', 'AH', 'L', 'AE', 'JH', 'M', 'IY']]\n",
      "supposed label: [['萨', '阿', '德', '穆', '罕', '默', '德', '本', ' ', '特', '夫', '拉', ' ', '阿', '贾', '米']]\n",
      "predict label:阿 尔 尔 拉 罕 默 德     阿 阿 卜 尔 \n",
      "\n",
      "input: [['ER', 'L', 'G', 'OW', 'R', 'AY']]\n",
      "supposed label: [['高', '里', '伯', '爵']]\n",
      "predict label:阿 克 克 特 \n",
      "\n",
      "input: [['AH', 'L', 'AH', 'N', 'G', 'AH', 'IH', 'L', 'AH', 'NG', 'K', 'AE', 'M', 'B', 'AH']]\n",
      "supposed label: [['伊', '隆', '加', '伊', '伦', '坎', '巴']]\n",
      "predict label:阿 万 多 \n",
      "\n",
      "input: [['EY', 'AY', 'IY', 'AH', 'L', 'IY']]\n",
      "supposed label: [['艾', '伊', '亚', '利']]\n",
      "predict label:阿 哈 万 \n",
      "\n",
      "input: [['V', 'AA', 'S', 'AH', 'L', 'M', 'IH', 'L', 'IY', 'AH']]\n",
      "supposed label: [['瓦', '西', '里', '米', '列', '亚']]\n",
      "predict label:阿 尔 里 克 尔 克 历 \n",
      "\n",
      "input: [['AE', 'L', 'F', 'AE', 'N', 'D', 'AH', 'G', 'AH']]\n",
      "supposed label: [['阿', '尔', '凡', '德', '加']]\n",
      "predict label:阿 尔 \n",
      "\n",
      "input: [['AE', 'R', 'IY', 'AH', 'T', 'G', 'JH', 'IH', 'R', 'IY']]\n",
      "supposed label: [['阿', '留', '湾']]\n",
      "predict label:阿 尔 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "LOSS =[]\n",
    "\n",
    "for q in range(epoch):\n",
    "    total_loss = 0\n",
    "    lasttime = time.time()\n",
    "    \n",
    "    for w in range(0, len(english_phoeneme) - batch_size, batch_size):  \n",
    "        \n",
    "        #shuffle the batch\n",
    "        combine = list(zip(english_phoeneme,chinese))\n",
    "        random.shuffle(combine)\n",
    "        batch_english_phoneme,batch_chinese = zip(*combine)\n",
    "        \n",
    "        _, losses = sess.run([optimizer, loss], \n",
    "                             feeding(batch_english_phoneme[w: w + batch_size], batch_chinese[w: w + batch_size]))\n",
    "        \n",
    "        total_loss += losses\n",
    "        \n",
    "    total_loss = total_loss / ((len(english_phoeneme) - batch_size) / (batch_size * 1.0))\n",
    "    LOSS.append(total_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (q + 1) % 10 == 0:\n",
    "        print('epoch: ' + str(q + 1) + ', total loss: ' + str(total_loss) + ', s/epoch: ' + str(time.time() - lasttime))\n",
    "        for i in range(10):\n",
    "            rand = np.random.randint(len(english_phoeneme))\n",
    "            test = feeding(english_phoeneme[rand: rand + 1], chinese[rand: rand + 1])\n",
    "            predict = sess.run(decoder_prediction, test)\n",
    "            print('input: ' + str(english_phoeneme[rand: rand + 1]))\n",
    "            print('supposed label: ' + str(chinese[rand: rand + 1]))\n",
    "            print('predict label:' + str(label_to_chinese(predict)) + '\\n')\n",
    "        print('#######################next 10 epoch#############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
