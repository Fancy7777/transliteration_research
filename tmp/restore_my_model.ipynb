{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get in the data\n",
    "chinese = []\n",
    "english_phoeneme = []\n",
    "with open('dataset.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        chinese.append(temp[1])\n",
    "        english_phoeneme.append(temp[2])\n",
    "english_phoeneme = [phoeneme.split('\\n')[0].split() for phoeneme in english_phoeneme]\n",
    "chinese = [list(word) for word in chinese]\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')\n",
    "def prepare_vocab_input():\n",
    "    vocab_inputs = []\n",
    "    with open('./english_phoneme_vocabulary_output.txt') as file:\n",
    "        for line in file:\n",
    "            vocab_inputs.append(line.split('\\n')[0])\n",
    "    vocab_inputs.remove('_PAD')\n",
    "    vocab_inputs.remove('_GO')\n",
    "    vocab_inputs.remove('_EOS')\n",
    "    vocab_inputs.remove('_UNK')\n",
    "    vocab_inputs = ['PAD', 'EOS'] + (vocab_inputs)\n",
    "    return vocab_inputs\n",
    "def prepare_vocab_predict():\n",
    "    vocab_predict = list(chinese_id_dict.keys())\n",
    "    return vocab_predict\n",
    "# prepare chinese_id_dict\n",
    "chinese_id_list = []\n",
    "with open('./chinese_vocabulary.txt') as file7:\n",
    "    for line in file7:\n",
    "        chinese_id_list.append(line.split('\\n')[0])\n",
    "chinese_id_list = ['PAD', 'EOS'] + chinese_id_list\n",
    "chinese_id_dict = {}\n",
    "for i in range(len(chinese_id_list)):\n",
    "    chinese_id_dict[chinese_id_list[i]] = i\n",
    "# finish preparation for chinese_id_dict\n",
    "vocab_inputs = prepare_vocab_input()\n",
    "vocab_predict = prepare_vocab_predict()\n",
    "english_phoneme_dict = {}\n",
    "for i in range(len(vocab_inputs)):\n",
    "    english_phoneme_dict[vocab_inputs[i]] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feeding(inputs, labels):\n",
    "    inputs_int = []; predict_int = []\n",
    "    for i in range(len(inputs)):\n",
    "        single_input = []\n",
    "        single_predict = []\n",
    "        tmp_inputs = max([len(x) for x in inputs])\n",
    "        print(inputs)\n",
    "        print(tmp_inputs)\n",
    "        tmp_labels = max([len(x) for x in labels])\n",
    "        tmp = max(tmp_inputs,tmp_labels)\n",
    "\n",
    "        for x in range(tmp):\n",
    "            try:\n",
    "                single_input += [english_phoneme_dict[inputs[i][x]]]\n",
    "            except:\n",
    "                single_input += [0]\n",
    "        for x in range(tmp):\n",
    "            try:\n",
    "                single_predict += [chinese_id_dict[labels[i][x]]]\n",
    "            except:\n",
    "                single_input +=[0]\n",
    "        inputs_int.append(single_input); predict_int.append(single_predict)\n",
    "        print(inputs_int)\n",
    "        print(predict_int)\n",
    "    enc_input, _ = helpers.batch(inputs_int)\n",
    "    dec_target, _ = helpers.batch([(sequence) + [1] for sequence in predict_int])\n",
    "    dec_input, _ = helpers.batch([[1] + (sequence) for sequence in inputs_int])\n",
    "    \n",
    "    return {encoder_inputs: enc_input, decoder_inputs: dec_input, decoder_targets: dec_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "greatestvalue_predict = 42\n",
    "encoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_targets = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "\n",
    "\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_inputs), greatestvalue_predict]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_predict), greatestvalue_predict]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(encoder_embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(decoder_embeddings, decoder_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden = 280\n",
    "\n",
    "cells = []\n",
    "for _ in range(2):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(hidden), input_keep_prob=0.5)\n",
    "    cells.append(cell)\n",
    "multicell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=False)\n",
    "\n",
    "_, encoder_final_state = tf.nn.dynamic_rnn(multicell, encoder_inputs_embedded,\n",
    "                                           dtype = tf.float32, time_major = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_cells = []\n",
    "\n",
    "for _ in range(2):\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(hidden), input_keep_prob=0.5)\n",
    "    decoder_cells.append(cell)\n",
    "decoder_multicell = tf.contrib.rnn.MultiRNNCell(decoder_cells, state_is_tuple=False)\n",
    "\n",
    "# declare a scope for our decoder, later tensorflow will confuse\n",
    "decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(decoder_multicell, decoder_inputs_embedded,\n",
    "                                                             initial_state=encoder_final_state,\n",
    "                                                             dtype=tf.float32, time_major=True, scope='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, len(vocab_predict))\n",
    "\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(decoder_targets, depth=len(vocab_predict), dtype=tf.float32),\n",
    "            logits=decoder_logits)\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(0.003).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "sess = tf.Session(config=config)\n",
    "saver = tf.train.import_meta_graph('./model/model.ckpt.meta')\n",
    "saver.restore(sess, \"./model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['M', 'UH', 'HH', 'AA', 'M', 'AH', 'D', 'K', 'AH', 'L', 'IY', 'F', 'AH', 'AE', 'L', 'K', 'IH', 'N', 'D', 'IY'], ['T', 'S', 'AA', 'G', 'T', 'AA', 'CH', 'AH', 'R', 'AH', 'N', 'L', 'AA', 'HH', 'Y', 'UW', 'AH', 'Z']]\n",
      "20\n",
      "[[8, 37, 24, 12, 8, 2, 9, 10, 2, 6, 7, 26, 2, 4, 6, 10, 15, 3, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[775, 634, 1055, 489, 215, 15, 941, 149, 465]]\n",
      "[['M', 'UH', 'HH', 'AA', 'M', 'AH', 'D', 'K', 'AH', 'L', 'IY', 'F', 'AH', 'AE', 'L', 'K', 'IH', 'N', 'D', 'IY'], ['T', 'S', 'AA', 'G', 'T', 'AA', 'CH', 'AH', 'R', 'AH', 'N', 'L', 'AA', 'HH', 'Y', 'UW', 'AH', 'Z']]\n",
      "20\n",
      "[[8, 37, 24, 12, 8, 2, 9, 10, 2, 6, 7, 26, 2, 4, 6, 10, 15, 3, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [13, 11, 12, 21, 13, 12, 32, 2, 5, 2, 3, 6, 12, 24, 29, 22, 2, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[775, 634, 1055, 489, 215, 15, 941, 149, 465], [1439, 728, 52, 353, 214, 997, 1037, 1301, 1446]]\n"
     ]
    }
   ],
   "source": [
    "feed=feeding(english_phoeneme[2: 4], chinese[2: 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[8, 37, 24, 12, 8, 2, 9, 10, 2, 6, 7, 26, 2, 4, 6, 10, 15, 3, 9, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [13, 11, 12, 21, 13, 12, 32, 2, 5, 2, 3, 6, 12, 24, 29, 22, 2, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['M', 'UH', 'HH', 'AA', 'M', 'AH', 'D', 'K', 'AH', 'L', 'IY', 'F', 'AH', 'AE', 'L', 'K', 'IH', 'N', 'D', 'IY'], ['T', 'S', 'AA', 'G', 'T', 'AA', 'CH', 'AH', 'R', 'AH', 'N', 'L', 'AA', 'HH', 'Y', 'UW', 'AH', 'Z']] [['穆', '罕', '默', '德', '哈', '利', '法', '金', '迪'], ['朝', '格', '特', '奥', '其', '尔', '洛', '呼', '兹']]\n"
     ]
    }
   ],
   "source": [
    "print(english_phoeneme[2: 4], chinese[2: 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict = sess.run(decoder_prediction,feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1113, 1113],\n",
       "       [1058, 1113],\n",
       "       [ 646, 1113],\n",
       "       [ 646,  409],\n",
       "       [ 158, 1371],\n",
       "       [ 158,  534],\n",
       "       [ 759, 1445],\n",
       "       [ 600, 1083],\n",
       "       [ 743, 1083],\n",
       "       [ 743,  584],\n",
       "       [ 743, 1083],\n",
       "       [ 747, 1083],\n",
       "       [ 926,  304],\n",
       "       [ 810, 1502],\n",
       "       [ 723, 1083],\n",
       "       [1348, 1083],\n",
       "       [ 135, 1082],\n",
       "       [ 135, 1082],\n",
       "       [1246, 1502],\n",
       "       [1246,  884],\n",
       "       [1246,  884],\n",
       "       [1246,  884],\n",
       "       [1207,  884],\n",
       "       [ 920,  884],\n",
       "       [ 253,  253],\n",
       "       [ 253,  952],\n",
       "       [ 253,  594],\n",
       "       [ 253,  253],\n",
       "       [ 253,  253],\n",
       "       [ 594,  253],\n",
       "       [ 594, 1269],\n",
       "       [ 594,  240]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['AE', 'K', 'AH', 'K', 'AY', 'JH']], [['阿', '卡', '基', '耶']])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_phoeneme[1: 2], chinese[1: 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tf.Tensor 'Placeholder:0' shape=(?, ?) dtype=int32>: array([[ 8, 13],\n",
       "        [37, 11],\n",
       "        [24, 12],\n",
       "        [12, 21],\n",
       "        [ 8, 13],\n",
       "        [ 2, 12],\n",
       "        [ 9, 32],\n",
       "        [10,  2],\n",
       "        [ 2,  5]], dtype=int32),\n",
       " <tf.Tensor 'Placeholder_2:0' shape=(?, ?) dtype=int32>: array([[ 1,  1],\n",
       "        [ 8, 13],\n",
       "        [37, 11],\n",
       "        [24, 12],\n",
       "        [12, 21],\n",
       "        [ 8, 13],\n",
       "        [ 2, 12],\n",
       "        [ 9, 32],\n",
       "        [10,  2],\n",
       "        [ 2,  5]], dtype=int32),\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?, ?) dtype=int32>: array([[ 775, 1439],\n",
       "        [ 634,  728],\n",
       "        [1055,   52],\n",
       "        [ 489,  353],\n",
       "        [ 215,  214],\n",
       "        [  15,  997],\n",
       "        [ 941, 1037],\n",
       "        [ 149, 1301],\n",
       "        [ 465, 1446],\n",
       "        [   1,    1]], dtype=int32)}"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_chinese(label):\n",
    "    chinese = ''\n",
    "    for i in range(len(label)):\n",
    "        if label[i][0] == 0 or label[i][0] == 1:\n",
    "            continue\n",
    "        chinese += vocab_predict[label[i][0]] + ' '\n",
    "    return chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6, 17, 8, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[872, 1037, 1342, 797, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['AH', 'L', 'OW', 'M', 'IY', 'AH']]\n",
      "supposed label: [['阿', '洛', '米', '亚']]\n",
      "predict label:筱 长 长 吴 吴 婉 婉 荻 尔 溏 蕾 溏 溏 溏 杰 杰 杰 白 蕾 腊 白 腊 腊 础 炎 腊 柄 柄 杰 仓 仓 仓 平 蕾 蕾 蕾 蕾 挽 挽 溏 溏 挽 \n",
      "\n",
      "[[18, 6, 7, 2, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[872, 15, 353, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['EY', 'L', 'IY', 'AH', 'T']]\n",
      "supposed label: [['阿', '利', '奥']]\n",
      "predict label:长 芭 芭 芭 芭 芭 芭 芭 寿 复 杰 杰 腊 腊 腊 腊 腊 腊 仙 溏 溏 复 腊 溏 溏 溏 溏 溏 溏 溏 溏 挽 腊 蕾 蕾 蕾 蕾 复 蕾 蕾 蕾 蕾 \n",
      "\n",
      "[[4, 6, 10, 17, 14, 2, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[872, 997, 1126, 842, 997, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['AE', 'L', 'K', 'OW', 'B', 'AH', 'R']]\n",
      "supposed label: [['阿', '尔', '科', '韦', '尔']]\n",
      "predict label:长 聪 聪 明 旃 旃 L 芥 芥 貂 炎 炎 炎 蕾 蕾 蕾 难 难 难 难 腊 杰 仓 腊 腊 平 仓 柄 柄 平 平 君 挽 咀 咀 锐 腊 柄 蕾 敢 复 咀 \n",
      "\n",
      "[[23, 12, 3, 17, 9, 12, 3, 2, 24, 29, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[403, 447, 353, 62, 434, 340, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['JH', 'AA', 'N', 'OW', 'D', 'AA', 'N', 'AH', 'HH', 'Y', 'UW']]\n",
      "supposed label: [['约', '翰', '奥', '多', '诺', '休']]\n",
      "predict label:长 长 长 长 桑 桑 - 桑 桑 仑 笏 迈 尺 胶 蓬 胶 溏 溏 溏 溏 溏 白 白 白 白 挽 咀 咀 胶 挽 挽 蓓 仓 仓 挽 仓 挽 挽 挽 锐 锐 锐 \n",
      "\n",
      "[[39, 7, 6, 8, 16, 3, 15, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[808, 812, 622, 218, 203, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['ZH', 'IY', 'L', 'M', 'EH', 'N', 'IH', 'JH']]\n",
      "supposed label: [['吉', '勒', '梅', '那', '热']]\n",
      "predict label:罗 恬 长 坊 L 迓 迓 水 H 水 礁 尹   坊 坊 斐 斐 打 坊 打 打 敢 仓 君 挽 挽 君 蕾 君 君 仓 锐 复 蕾 腊 腊 蕾 锐 蕾 蕾 蕾 蕾 \n",
      "\n",
      "[[2, 6, 16, 10, 11, 2, 3, 9, 5, 7, 3, 11, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[797, 1135, 460, 478, 127, 849, 1435, 997, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['AH', 'L', 'EH', 'K', 'S', 'AH', 'N', 'D', 'R', 'IY', 'N', 'S', 'AH', 'L']]\n",
      "supposed label: [['亚', '历', '山', '大', '艾', '因', '塞', '尔']]\n",
      "predict label:筱 芭 斐 芭 斐 芭 芭 来 芭 芭 存 爵 爵 爵 爵 爵 爵 爵 眼 殿 殿 挽 挽 挽 挽 挽 蕾 蕾 蕾 蕾 蕾 蕾 蕾 蕾 挽 挽 仓 挽 仓 旋 挽 挽 \n",
      "\n",
      "[[24, 18, 14, 7, 23, 15, 21, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[215, 135, 808, 1429, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['HH', 'EY', 'B', 'IY', 'JH', 'IH', 'G', 'AH']]\n",
      "supposed label: [['哈', '比', '吉', '加']]\n",
      "predict label:荔 荔 ─ ─ ─ ─ 来 来 礁 礁 礁 迁 迁 迁 笛 嵋 杰 胶 挽 挽 胶 腊 挽 腊 挽 复 仓 挽 腊 腊 蕾 柄 挽 挽 柄 仓 锐 蕾 蕾 蕾 蕾 仓 \n",
      "\n",
      "[[20, 18, 6, 2, 5, 7, 7, 25, 2, 5, 16, 33, 29, 2, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[1415, 1031, 1193, 869, 1031, 1469, 1285, 939, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['V', 'EY', 'L', 'AH', 'R', 'IY', 'IY', 'P', 'AH', 'R', 'EH', 'SH', 'Y', 'AH', 'R', 'AH']]\n",
      "supposed label: [['瓦', '列', '里', '佩', '列', '什', '库', '拉']]\n",
      "predict label:聪 前 前 聪 聪 聪 聪 聪 与 与 泉 婷 婷 麟 婷 婷 婷 婷 婷 普 蔡 房 份 白 白 白 白 白 白 白 挽 白 挽 挽 挽 挽 挽 挽 挽 挽 挽 挽 \n",
      "\n",
      "[[16, 8, 27, 7, 14, 12, 5, 3, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[578, 1055, 1193, 237, 570, 1233, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['EH', 'M', 'ER', 'IY', 'B', 'AA', 'R', 'N', 'Z']]\n",
      "supposed label: [['埃', '默', '里', '巴', '恩', '斯']]\n",
      "predict label:长 长 丽 创 徒 徒 徒 徒 徒 徒 杰 阅 阅 L 挽 挽 挽 溏 溏 溏 溏 溏 溏 溏 复 复 复 复 复 锟 锐 挽 挽 咀 复 复 旋 复 复 腊 蕾 挽 \n",
      "\n",
      "[[2, 14, 30, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[872, 237, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "input: [['AH', 'B', 'AO', 'L', 'IY']]\n",
      "supposed label: [['阿', '巴', '利']]\n",
      "predict label:喷 长 来 来 来 来 来 来 来 嵋 蕾 蕾 蕾 胶 蕾 旋 旋 旋 旋 旋 旋 白 仓 仓 旋 仓 君 仓 谷 画 蕾 仓 仓 仓 白 白 白 白 君 挽 挽 挽 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(10):\n",
    "    rand = np.random.randint(len(english_phoeneme))\n",
    "    test = feeding(english_phoeneme[rand: rand+1], chinese[rand: rand+1])\n",
    "    predict = sess.run(decoder_prediction, test)\n",
    "    print('input: ' + str(english_phoeneme[rand: rand+1]))\n",
    "    print('supposed label: ' + str(chinese[rand: rand+1]))\n",
    "    print('predict label:' + str(label_to_chinese(predict))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27439"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<tf.Tensor 'Placeholder:0' shape=(?, ?) dtype=int32>: array([[ 8],\n",
      "       [37],\n",
      "       [24],\n",
      "       [12],\n",
      "       [ 8],\n",
      "       [ 2],\n",
      "       [ 9],\n",
      "       [10],\n",
      "       [ 2]], dtype=int32), <tf.Tensor 'Placeholder_2:0' shape=(?, ?) dtype=int32>: array([[ 1],\n",
      "       [ 8],\n",
      "       [37],\n",
      "       [24],\n",
      "       [12],\n",
      "       [ 8],\n",
      "       [ 2],\n",
      "       [ 9],\n",
      "       [10],\n",
      "       [ 2]], dtype=int32)}\n",
      "input: [['M', 'UH', 'HH', 'AA', 'M', 'AH', 'D', 'K', 'AH', 'L', 'IY', 'F', 'AH', 'AE', 'L', 'K', 'IH', 'N', 'D', 'IY']]\n",
      "supposed label: [['穆', '罕', '默', '德', '哈', '利', '法', '金', '迪']]\n",
      "predict label:迅 迅 顿 棉 棉 贴 棉 塔 塔 归 \n"
     ]
    }
   ],
   "source": [
    "test = feeding(english_phoeneme[2: 3], chinese[2: 3])\n",
    "predict = sess.run(decoder_prediction, test)\n",
    "print(test)\n",
    "print('input: ' + str(english_phoeneme[2: 3]))\n",
    "print('supposed label: ' + str(chinese[2: 3]))\n",
    "print('predict label:' + str(label_to_chinese(predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
