{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get in the data\n",
    "chinese = []\n",
    "english_phoeneme = []\n",
    "with open('dataset.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        chinese.append(temp[1])\n",
    "        english_phoeneme.append(temp[2])\n",
    "english_phoeneme = [phoeneme.split('\\n')[0].split() for phoeneme in english_phoeneme]\n",
    "chinese = [list(word) for word in chinese]\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')\n",
    "def prepare_vocab_input():\n",
    "    vocab_inputs = []\n",
    "    with open('./english_phoneme_vocabulary_output.txt') as file:\n",
    "        for line in file:\n",
    "            vocab_inputs.append(line.split('\\n')[0])\n",
    "    vocab_inputs.remove('_PAD')\n",
    "    vocab_inputs.remove('_GO')\n",
    "    vocab_inputs.remove('_EOS')\n",
    "    vocab_inputs.remove('_UNK')\n",
    "    #vocab_inputs = ['PAD', 'EOS'] + (vocab_inputs)\n",
    "    vocab_inputs =  (vocab_inputs)+['PAD']\n",
    "    return vocab_inputs\n",
    "def prepare_vocab_predict():\n",
    "    vocab_predict = list(chinese_id_dict.keys())\n",
    "    return vocab_predict\n",
    "# prepare chinese_id_dict\n",
    "chinese_id_list = []\n",
    "with open('./chinese_vocabulary.txt') as file7:\n",
    "    for line in file7:\n",
    "        chinese_id_list.append(line.split('\\n')[0])\n",
    "chinese_id_list =  chinese_id_list+['PAD']\n",
    "chinese_id_dict = {}\n",
    "for i in range(len(chinese_id_list)):\n",
    "    chinese_id_dict[chinese_id_list[i]] = i\n",
    "# finish preparation for chinese_id_dict\n",
    "vocab_inputs = prepare_vocab_input()\n",
    "vocab_predict = prepare_vocab_predict()\n",
    "english_phoneme_dict = {}\n",
    "for i in range(len(vocab_inputs)):\n",
    "    english_phoneme_dict[vocab_inputs[i]] = i\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#128,256\n",
    "batch_size = 128\n",
    "embedding_size = 256\n",
    "encoder_hidden_units = 32\n",
    "decoder_hidden_units = 32\n",
    "epoch = 10\n",
    "encoder_max_length = 39\n",
    "decoder_max_length =75\n",
    "\n",
    "#placeholders pass data in\n",
    "encoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_targets = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "decoder_inputs = tf.placeholder(shape = (None, None), dtype = tf.int32)\n",
    "\n",
    "#variables\n",
    "#size should be [max_time,batch_size] try batch_size here\n",
    "encoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_inputs), embedding_size]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([len(vocab_predict), embedding_size]\n",
    "                                                   , -1.0, 1.0), dtype = tf.float32)\n",
    "encoder_inputs_embedded = tf.nn.embedding_lookup(encoder_embeddings, encoder_inputs)\n",
    "decoder_inputs_embedded = tf.nn.embedding_lookup(decoder_embeddings, decoder_inputs)\n",
    "\n",
    "with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "    #initial encoder cell to be LSTM\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)\n",
    "    encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded,\n",
    "                                                dtype=tf.float32, time_major=False)\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    #initial decoder to be also LSTMcell\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(decoder_hidden_units)\n",
    "    decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(decoder_cell, decoder_inputs_embedded,\n",
    "                            initial_state=encoder_final_state,dtype=tf.float32, time_major=False,scope='decoder')\n",
    "\n",
    "    \n",
    "decoder_logits = tf.contrib.layers.linear(decoder_outputs, len(vocab_predict))\n",
    "# [None, seq_len, vocab_size]\n",
    "\n",
    "#在第二个维度上取argmax\n",
    "decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.one_hot(decoder_targets, depth=len(vocab_predict), dtype=tf.float32),\n",
    "            logits=decoder_logits)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "# def sent_len(sentence):\n",
    "#     '''\n",
    "#     Args:\n",
    "#         sentence: [None, max_sentence_length]\n",
    "#     Returns:\n",
    "#         length: [None], the actual length of each sentence in the batch\n",
    "#     '''\n",
    "#     used = tf.sign(tf.abs(sentence))\n",
    "#     length = tf.reduce_sum(used, reduction_indices=1)\n",
    "#     length = tf.cast(x=length, dtype=tf.int32)\n",
    "#     return length\n",
    "\n",
    "# cross_entropy_mask = tf.sequence_mask(\n",
    "#     lengths=sent_len(decoder_targets),\n",
    "#     maxlen=decoder_max_length + 1,\n",
    "#     dtype=tf.float32,\n",
    "# )\n",
    "# # [None, max_sentence_length]\n",
    "\n",
    "# loss = tf.contrib.seq2seq.sequence_loss(\n",
    "#     logits=decoder_logits, # [None, max_sentence_length, tag_set_size]\n",
    "#     targets=decoder_targets, # [None, max_sentence_length]\n",
    "#     weights=cross_entropy_mask,\n",
    "#     average_across_timesteps=True,\n",
    "#     average_across_batch=True,\n",
    "#     name=\"seq2seq_sequence_loss\",\n",
    "# )\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(inputs, max_sequence_length = None):\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape = [batch_size, max_sequence_length], dtype = np.int32)\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_batch_major, sequence_lengths\n",
    "\n",
    "def feeding(inputs, labels):\n",
    "    inputs_int = []; predict_int = []\n",
    "    for i in range(len(inputs)):\n",
    "        single_input = []\n",
    "        single_predict = []\n",
    "        for x in range(encoder_max_length):\n",
    "            try:\n",
    "                single_input += [english_phoneme_dict[inputs[i][x]]]\n",
    "            except:\n",
    "                single_input += [0]\n",
    "        for x in range(decoder_max_length):\n",
    "            try:\n",
    "                single_predict += [chinese_id_dict[labels[i][x]]]\n",
    "            except:\n",
    "                single_predict += [0]\n",
    "        inputs_int.append(single_input); predict_int.append(single_predict)\n",
    "            \n",
    "    enc_input, _ = batch(inputs_int)\n",
    "    dec_target, _ = batch([(sequence) + [1] for sequence in predict_int])\n",
    "    dec_input, _ = batch([[1] + (sequence) for sequence in predict_int])\n",
    "    #print(enc_input.shape)\n",
    "    return {encoder_inputs: enc_input, decoder_inputs: dec_input, decoder_targets: dec_target}\n",
    "\n",
    "def label_to_chinese(label):\n",
    "    chinese = ''\n",
    "    for i in range(len(label)):\n",
    "        for j in range(len(label[i])):\n",
    "            if label[i][j] == 0 or label[i][j] == 1:\n",
    "                continue\n",
    "            chinese += vocab_predict[label[i][j]] + ' '\n",
    "    return chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7154107d9a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         _, losses = sess.run([optimizer, loss], \n\u001b[0;32m---> 18\u001b[0;31m                              feeding(batch_english_phoneme[w: w + batch_size], batch_chinese[w: w + batch_size]))\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/DerekWang/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "LOSS =[]\n",
    "\n",
    "for q in range(epoch):\n",
    "    total_loss = 0\n",
    "    lasttime = time.time()\n",
    "    \n",
    "    for w in range(0, len(english_phoeneme) - batch_size, batch_size):  \n",
    "        \n",
    "        #shuffle the batch\n",
    "        combine = list(zip(english_phoeneme,chinese))\n",
    "        random.shuffle(combine)\n",
    "        batch_english_phoneme,batch_chinese = zip(*combine)\n",
    "        \n",
    "        _, losses = sess.run([optimizer, loss], \n",
    "                             feeding(batch_english_phoneme[w: w + batch_size], batch_chinese[w: w + batch_size]))\n",
    "        \n",
    "        total_loss += losses\n",
    "        \n",
    "    total_loss = total_loss / ((len(english_phoeneme) - batch_size) / (batch_size * 1.0))\n",
    "    LOSS.append(total_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (q + 1) % 10 == 0:\n",
    "        print('epoch: ' + str(q + 1) + ', total loss: ' + str(total_loss) + ', s/epoch: ' + str(time.time() - lasttime))\n",
    "        for i in range(10):\n",
    "            rand = np.random.randint(len(english_phoeneme))\n",
    "            test = feeding(english_phoeneme[rand: rand + 1], chinese[rand: rand + 1])\n",
    "            predict = sess.run(decoder_prediction, test)\n",
    "            print('input: ' + str(english_phoeneme[rand: rand + 1]))\n",
    "            print('supposed label: ' + str(chinese[rand: rand + 1]))\n",
    "            print('predict label:' + str(label_to_chinese(predict)) + '\\n')\n",
    "        print('#######################next 10 epoch#############################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
