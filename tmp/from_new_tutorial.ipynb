{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get in the data\n",
    "chinese = []\n",
    "english_phoeneme = []\n",
    "with open('dataset.txt') as f:\n",
    "    for line in f:\n",
    "        temp = line.split('\\t')\n",
    "        chinese.append(temp[1])\n",
    "        english_phoeneme.append(temp[2])\n",
    "english_phoeneme = [phoeneme.split('\\n')[0].split() for phoeneme in english_phoeneme]\n",
    "chinese = [list(word) for word in chinese]\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')\n",
    "def prepare_vocab_input():\n",
    "    vocab_inputs = []\n",
    "    with open('./english_phoneme_vocabulary_output.txt') as file:\n",
    "        for line in file:\n",
    "            vocab_inputs.append(line.split('\\n')[0])\n",
    "    vocab_inputs.remove('_PAD')\n",
    "    vocab_inputs.remove('_GO')\n",
    "    vocab_inputs.remove('_EOS')\n",
    "    vocab_inputs.remove('_UNK')\n",
    "    #vocab_inputs = ['PAD', 'EOS'] + (vocab_inputs)\n",
    "    vocab_inputs =  (vocab_inputs)+['PAD']\n",
    "    return vocab_inputs\n",
    "def prepare_vocab_predict():\n",
    "    vocab_predict = list(chinese_id_dict.keys())\n",
    "    return vocab_predict\n",
    "# prepare chinese_id_dict\n",
    "chinese_id_list = []\n",
    "with open('./chinese_vocabulary.txt') as file7:\n",
    "    for line in file7:\n",
    "        chinese_id_list.append(line.split('\\n')[0])\n",
    "chinese_id_list =  chinese_id_list+['PAD']\n",
    "chinese_id_dict = {}\n",
    "for i in range(len(chinese_id_list)):\n",
    "    chinese_id_dict[chinese_id_list[i]] = i\n",
    "# finish preparation for chinese_id_dict\n",
    "vocab_inputs = prepare_vocab_input()\n",
    "vocab_predict = prepare_vocab_predict()\n",
    "english_phoneme_dict = {}\n",
    "for i in range(len(vocab_inputs)):\n",
    "    english_phoneme_dict[vocab_inputs[i]] = i\n",
    "for ele in chinese:\n",
    "    if ' ' in ele:\n",
    "        ele.remove(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_phoneme_dict - char2numX\n",
    "chinese_id_dict - char2numY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "#prepare the english input\n",
    "english_now = dict(zip(english_phoneme_dict.values(),english_phoneme_dict.keys()))\n",
    "max_len = max([len(ele) for ele in english_phoeneme])\n",
    "print(max_len)\n",
    "\n",
    "english = [[english_phoneme_dict['PAD']]*(max_len - len(ele))+[english_phoneme_dict[phoneme] for phoneme in ele] for ele in english_phoeneme]\n",
    "english = np.array(english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "#prepare the chinese output\n",
    "chinese_now = dict(zip(chinese_id_dict.values(),chinese_id_dict.keys()))\n",
    "max_len_chi = max([len(ele) for ele in chinese])\n",
    "print(max_len_chi)\n",
    "\n",
    "chinese = [[chinese_id_dict['PAD']]*(max_len_chi - len(ele))+[chinese_id_dict[char] for char in ele] for ele in chinese]\n",
    "chinese = np.array(chinese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_data(x, y, batch_size):\n",
    "    shuffle = np.random.permutation(len(x))\n",
    "    start = 0\n",
    "#     from IPython.core.debugger import Tracer; Tracer()()\n",
    "    x = x[shuffle]\n",
    "    y = y[shuffle]\n",
    "    while start + batch_size <= len(x):\n",
    "        yield x[start:start+batch_size], y[start:start+batch_size]\n",
    "        start += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq_length = len(english[0])\n",
    "y_seq_length = len(chinese[0])- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 128\n",
    "nodes = 32\n",
    "embed_size = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Tensor where we will feed the data into graph\n",
    "inputs = tf.placeholder(tf.int32, (None, x_seq_length), 'inputs')\n",
    "outputs = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "targets = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "\n",
    "# Embedding layers\n",
    "input_embedding = tf.Variable(tf.random_uniform((len(english_phoneme_dict), embed_size), -1.0, 1.0), name='enc_embedding')\n",
    "output_embedding = tf.Variable(tf.random_uniform((len(chinese_id_dict), embed_size), -1.0, 1.0), name='dec_embedding')\n",
    "date_input_embed = tf.nn.embedding_lookup(input_embedding, inputs)\n",
    "date_output_embed = tf.nn.embedding_lookup(output_embedding, outputs)\n",
    "\n",
    "with tf.variable_scope(\"encoding\") as encoding_scope:\n",
    "    lstm_enc = tf.contrib.rnn.BasicLSTMCell(nodes)\n",
    "    _, last_state = tf.nn.dynamic_rnn(lstm_enc, inputs=date_input_embed, dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "    lstm_dec = tf.contrib.rnn.BasicLSTMCell(nodes)\n",
    "    dec_outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=date_output_embed, initial_state=last_state)\n",
    "#connect outputs to \n",
    "logits = tf.contrib.layers.fully_connected(dec_outputs, num_outputs=len(chinese_id_dict), activation_fn=None) \n",
    "with tf.name_scope(\"optimization\"):\n",
    "    # Loss function\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.RMSPropOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(english, chinese, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Loss:  0.905 Accuracy: 0.8427 Epoch duration: 73.806s\n",
      "Epoch   1 Loss:  0.862 Accuracy: 0.8427 Epoch duration: 74.474s\n",
      "Epoch   2 Loss:  0.786 Accuracy: 0.8563 Epoch duration: 73.188s\n",
      "Epoch   3 Loss:  0.786 Accuracy: 0.8481 Epoch duration: 72.964s\n",
      "Epoch   4 Loss:  0.757 Accuracy: 0.8555 Epoch duration: 73.219s\n",
      "Epoch   5 Loss:  0.746 Accuracy: 0.8627 Epoch duration: 74.112s\n",
      "Epoch   6 Loss:  0.744 Accuracy: 0.8596 Epoch duration: 73.939s\n",
      "Epoch   7 Loss:  0.734 Accuracy: 0.8649 Epoch duration: 72.941s\n",
      "Epoch   8 Loss:  0.741 Accuracy: 0.8618 Epoch duration: 72.730s\n",
      "Epoch   9 Loss:  0.755 Accuracy: 0.8567 Epoch duration: 73.212s\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 10\n",
    "for epoch_i in range(epochs):\n",
    "    start_time = time.time()\n",
    "    for batch_i, (source_batch, target_batch) in enumerate(batch_data(X_train, y_train, batch_size)):\n",
    "        _, batch_loss, batch_logits = sess.run([optimizer, loss, logits],\n",
    "            feed_dict = {inputs: source_batch,\n",
    "             outputs: target_batch[:, :-1],\n",
    "             targets: target_batch[:, 1:]})\n",
    "    accuracy = np.mean(batch_logits.argmax(axis=-1) == target_batch[:,1:])\n",
    "    print('Epoch {:3} Loss: {:>6.3f} Accuracy: {:>6.4f} Epoch duration: {:>6.3f}s'.format(epoch_i, batch_loss, \n",
    "                                                                      accuracy, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is:  0.853\n"
     ]
    }
   ],
   "source": [
    "source_batch, target_batch = next(batch_data(X_test, y_test, batch_size))\n",
    "\n",
    "dec_input = np.zeros((len(source_batch), 1)) + chinese_id_dict['PAD']\n",
    "for i in range(y_seq_length):\n",
    "    batch_logits = sess.run(logits,\n",
    "                feed_dict = {inputs: source_batch,\n",
    "                 outputs: dec_input})\n",
    "    prediction = batch_logits[:,-1].argmax(axis=-1)\n",
    "    dec_input = np.hstack([dec_input, prediction[:,None]])\n",
    "    \n",
    "print('Accuracy on test set is: {:>6.3f}'.format(np.mean(dec_input == target_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIHLYAHMNAEGWAHBSAYFAYN => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "RIHKAARDOWROWHHAAS => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "AEMBAHLAHPEYSOW => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD阿尔特拉斯\n",
      "HHOWZEYKOWDAHRCHPLAENAHZ => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "VAEKLAAVHHAORAHCHEHK => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "MOWHHAAMEHDAALIYAEBTAHHHIY => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "WIHLYAHMJHEYMZPEHRIY => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n",
      "MEHRIYERUHRK => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD阿尔\n",
      "AHIHNZAHTAWN => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD阿尔特拉斯\n",
      "MUWSTAAFAHKAHLEHMLIY => PADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPADPAD\n"
     ]
    }
   ],
   "source": [
    "num_preds = 10\n",
    "source_chars = [[english_now[l] for l in sent if english_now[l]!=\"PAD\"] for sent in source_batch[:num_preds]]\n",
    "dest_chars = [[chinese_now[l] for l in sent] for sent in dec_input[:num_preds, 1:]]\n",
    "\n",
    "for date_in, date_out in zip(source_chars, dest_chars):\n",
    "    print(''.join(date_in)+' => '+''.join(date_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
